{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.18.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (0.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.0 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import load_iris\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=2,figsize=(16,12),dpi=80,facecolor='w',edgecolor='k')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacking():\n",
    "  # define the base models\n",
    "  level0 = list()\n",
    "  level0.append(('lr', LogisticRegression()))\n",
    "  level0.append(('knn', KNeighborsClassifier()))\n",
    "  level0.append(('cart', DecisionTreeClassifier()))\n",
    "  level0.append(('svm', SVC()))\n",
    "  level0.append(('bayes', GaussianNB()))\n",
    "  # define meta learner model\n",
    "  level1 = LogisticRegression()\n",
    "  # define the stacking ensemble\n",
    "  model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = dict()\n",
    "    models['LogisticRegression'] = LogisticRegression()\n",
    "    models['KNeighborsClassifier'] = KNeighborsClassifier()\n",
    "    models['Decision Tree'] = DecisionTreeClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['GaussianNB'] = GaussianNB()\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=1)\n",
    "    scores = cross_val_score(model,X,y,scoring='accuracy',cv=cv,n_jobs=-1,error_score='raise')\n",
    "    scores1 = cross_val_score(model,X1,y1,scoring='accuracy',cv=cv,n_jobs=-1,error_score='raise')\n",
    "    return (scores,scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset.iloc[:,[2,5,6,7,9,11,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before imputing  Pclass        0\n",
      "Age         177\n",
      "SibSp         0\n",
      "Parch         0\n",
      "Fare          0\n",
      "Embarked      2\n",
      "Survived      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputing  0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    2\n",
      "6    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values before imputing \",a.isna().sum())\n",
    "imp = SimpleImputer(missing_values=np.nan,strategy=\"mean\")\n",
    "b = a.values\n",
    "b[:,[0,1,2,3,4]] = imp.fit_transform(b[:,[0,1,2,3,4]])\n",
    "print(\"\\nMissing values after imputing \",pd.DataFrame(b).isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after removal\n",
      " Pclass        0\n",
      "Age         177\n",
      "SibSp         0\n",
      "Parch         0\n",
      "Fare          0\n",
      "Embarked      2\n",
      "Survived      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after removal\n",
      " Pclass      0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "Survived    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values after removal\\n\",a.isna().sum())\n",
    "a = pd.DataFrame(a)\n",
    "a = a.dropna()\n",
    "print(\"\\nMissing values after removal\\n\",pd.DataFrame(a).isna().sum())\n",
    "a = a.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After label encoding  [[3 22.0 1 ... 7.25 'S' 0]\n",
      " [1 38.0 1 ... 71.2833 'C' 1]\n",
      " [3 26.0 0 ... 7.925 'S' 1]\n",
      " ...\n",
      " [1 19.0 0 ... 30.0 'S' 1]\n",
      " [1 26.0 0 ... 30.0 'C' 1]\n",
      " [3 32.0 0 ... 7.75 'Q' 0]]\n"
     ]
    }
   ],
   "source": [
    "lb = LabelEncoder()\n",
    "a[:,-1] = lb.fit_transform(a[:,-1])\n",
    "print(\"\\nAfter label encoding \",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After one hot encoding  [[0.0 0.0 1.0 ... 0 7.25 0]\n",
      " [1.0 0.0 0.0 ... 0 71.2833 1]\n",
      " [0.0 0.0 1.0 ... 0 7.925 1]\n",
      " ...\n",
      " [0.0 0.0 1.0 ... 0 30.0 1]\n",
      " [1.0 0.0 0.0 ... 0 30.0 1]\n",
      " [0.0 1.0 0.0 ... 0 7.75 0]]\n"
     ]
    }
   ],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    transformers = [(\"OneHot\",OneHotEncoder(),[5])],\n",
    "    remainder = 'passthrough'\n",
    ")\n",
    "a = transformer.fit_transform(a.tolist())\n",
    "print(\"\\nAfter one hot encoding \",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = a[:,:-1]\n",
    "y1 = a[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X1 = scaler.fit_transform(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = y1.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris().data, load_iris().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('LogisticRegression', LogisticRegression()), ('KNeighborsClassifier', KNeighborsClassifier()), ('Decision Tree', DecisionTreeClassifier()), ('svm', SVC()), ('GaussianNB', GaussianNB()), ('stacking', StackingClassifier(cv=5,\n",
      "                   estimators=[('lr', LogisticRegression()),\n",
      "                               ('knn', KNeighborsClassifier()),\n",
      "                               ('cart', DecisionTreeClassifier()),\n",
      "                               ('svm', SVC()), ('bayes', GaussianNB())],\n",
      "                   final_estimator=LogisticRegression()))])\n",
      "->LogisticRegression -> 0.964 (0.041) --- Iris dataset\n",
      "->LogisticRegression -> 0.712 (0.041) --- Titanic dataset\n",
      "->KNeighborsClassifier -> 0.964 (0.037) --- Iris dataset\n",
      "->KNeighborsClassifier -> 0.701 (0.035) --- Titanic dataset\n",
      "->Decision Tree -> 0.947 (0.056) --- Iris dataset\n",
      "->Decision Tree -> 0.662 (0.045) --- Titanic dataset\n",
      "->svm -> 0.964 (0.045) --- Iris dataset\n",
      "->svm -> 0.724 (0.044) --- Titanic dataset\n",
      "->GaussianNB -> 0.956 (0.047) --- Iris dataset\n",
      "->GaussianNB -> 0.669 (0.050) --- Titanic dataset\n",
      "->stacking -> 0.962 (0.041) --- Iris dataset\n",
      "->stacking -> 0.733 (0.041) --- Titanic dataset\n"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "print(models.items())\n",
    "results, names, results1 = list(),list(),list()\n",
    "for (name,model) in models.items():\n",
    "    scores, scores1 = evaluate_model(model)\n",
    "    results.append(scores)\n",
    "    results1.append(scores1)\n",
    "    names.append(name)\n",
    "    print('->%s -> %.3f (%.3f) --- Iris dataset' % (name,mean(scores),std(scores)))\n",
    "    print('->%s -> %.3f (%.3f) --- Titanic dataset' % (name,mean(scores1),std(scores1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
